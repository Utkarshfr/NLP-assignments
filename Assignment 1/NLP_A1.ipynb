{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS6320: Assignment 1\n",
        "\n",
        "#### Group - 32\n",
        "\n",
        "#### Authors:\n",
        " - Utkarsh Farkya - uxf220000\n",
        " - Sai Nikhil Vaddhi - sxv210095"
      ],
      "metadata": {
        "id": "VnDF1AoraxM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing modules for data preprocessing and storage"
      ],
      "metadata": {
        "id": "jSgLM2_R8GYg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVsKnWMP4coM",
        "outputId": "da35b279-fa04-4cd1-e3a6-497ea7dc17ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"wordnet\")\n",
        "import string\n",
        "from collections import defaultdict, Counter\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Intializing lemmatizer and punctuations constants"
      ],
      "metadata": {
        "id": "4SNgTSf58U-9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W-gWM6SH_kTM"
      },
      "outputs": [],
      "source": [
        "punctuations = string.punctuation\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retreiving training and validation data from the URLs"
      ],
      "metadata": {
        "id": "UZWVwAPW8Zor"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F6h1BHd-I8ly"
      },
      "outputs": [],
      "source": [
        "# Dataset URLs\n",
        "TRAINING_SET_URL = \"https://gitlab.com/utkarshfr/nlp-fall2023/-/raw/main/A1-dataset/train.txt\"\n",
        "VALIDATION_SET_URL = \"https://gitlab.com/utkarshfr/nlp-fall2023/-/raw/main/A1-dataset/val.txt\"\n",
        "\n",
        "# Get raw training data from the URL\n",
        "raw_training_data = urllib.request.urlopen(TRAINING_SET_URL).read().decode()\n",
        "raw_training_data_sentences = raw_training_data.split(\"\\n\")\n",
        "raw_training_data_sentences.pop()\n",
        "\n",
        "# Get test data from the URL\n",
        "raw_validation_data = urllib.request.urlopen(VALIDATION_SET_URL).read().decode()\n",
        "raw_validation_data_sentences = raw_validation_data.split(\"\\n\")\n",
        "\n",
        "raw_sample_data = \"the students like the assignment\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---RAW TRAINING DATA---\")\n",
        "raw_training_data[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "lI-gjVGK_jyP",
        "outputId": "30f7d0fd-f762-41a1-a9c2-55b70d07fa41"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---RAW TRAINING DATA---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I booked two rooms four months in advance at the Talbott . We were placed on the top floor next to the elevators , which are used all night long . When speaking to the front desk , I was told that they were simply honoring my request for an upper floor , which I had requested for a better view . I am looking at a brick wall , and getting no sleep . He also told me that they had received complaints before from guests on the 16th floor , and were aware of the noise problem . Why then did they place us on this floor when the hotel is not totally booked ? A request for an upper floor does not constitute placing someone on the TOP floor and using that request to justify this . If you decide to stay here , request a room on a lower floor and away from the elevator ! I spoke at length when booking my two rooms about my preferences . This is simply poor treatment of a guest whom they believed would not complain .\\nI LOVED this hotel . The room was so chic and trendy , the bed was comfortable , '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---RAW VALIDATION DATA---\")\n",
        "raw_validation_data[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "HtP41XUH_g2L",
        "outputId": "8c5883e4-ac3b-4c57-ae6a-176b8ed6fb2a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---RAW VALIDATION DATA---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I stayed for four nights while attending a conference . The hotel is in a great spot - easy walk to Michigan Ave shopping or Rush St. , but just off the busy streets . The room I had was spacious , and very well-appointed . The staff was friendly , and the fitness center , while not huge , was well-equipped and clean . I 've stayed at a number of hotels in Chicago , and this one is my favorite . Internet was n't free , but at $ 10 for 24 hours is cheaper than most business hotels , and it worked very well .\\nwe love the location and proximity to everything . The staff was very friendly and courteous . They were so nice to our 2.5 year old boy . got his backpack full of goodies the moment we arrived . We got free wifi and morning drinks by signing up for select guest program . Ca n't beat that ! the only minor issue is the elevator . we have to take 2 separate elevator trips to get to our room . It got a little annoying when we were going in and out often . Otherwise , it was a great sta\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing training and validation data"
      ],
      "metadata": {
        "id": "LcAwjgVl8gI6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZvAkuIWaLeck"
      },
      "outputs": [],
      "source": [
        "# Even though tokenized, contains words like 've, 'd which should be removed during preprocessing\n",
        "\n",
        "def tokenize_data(data):\n",
        "    sentence = word_tokenize(data)\n",
        "    # Adding <start> and <stop> token at the beginning and end of the sentence\n",
        "    sentence.insert(0, \"<start>\")\n",
        "    sentence.append(\"<stop>\")\n",
        "    return sentence\n",
        "\n",
        "def preprocess_sentence(data):\n",
        "    data = tokenize_data(data)\n",
        "    # removing punctuations, words starting with punctuations, and stopwords from the corpus\n",
        "    data = [token.lower() for token in data if (token ==\"<start>\" or token == \"<stop>\") or  (token not in punctuations and token[0] not in punctuations and token not in stopwords.words(\"english\"))]\n",
        "    # lemmatizing the words to get their root word, in most cases does not change th meaning the sentence\n",
        "    # helps in focusing on important words\n",
        "    data = [wordnet_lemmatizer.lemmatize(token) for token in data]\n",
        "    return data\n",
        "\n",
        "def preprocess_corpus(data, process_sentences = False):\n",
        "    if process_sentences:\n",
        "        data = [preprocess_sentence(e) for e in data]\n",
        "        return data\n",
        "    return preprocess_sentence(data)\n",
        "\n",
        "def flatten_sentences(data):\n",
        "    return [word for sentence in data for word in sentence]\n",
        "\n",
        "train_data_sentences = preprocess_corpus(raw_training_data_sentences, True)\n",
        "validation_data_sentences = preprocess_corpus(raw_validation_data_sentences, True)\n",
        "train_data = flatten_sentences(train_data_sentences)\n",
        "validation_data = flatten_sentences(validation_data_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---PREPROCESSED TRAINING DATA---\")\n",
        "print(train_data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EokyY7QZ_6mE",
        "outputId": "a3ad5cec-ad20-4070-df0e-69b5b402feb3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---PREPROCESSED TRAINING DATA---\n",
            "['<start>', 'i', 'booked', 'two', 'room', 'four', 'month', 'advance', 'talbott', 'we', 'placed', 'top', 'floor', 'next', 'elevator', 'used', 'night', 'long', 'when', 'speaking', 'front', 'desk', 'i', 'told', 'simply', 'honoring', 'request', 'upper', 'floor', 'i', 'requested', 'better', 'view', 'i', 'looking', 'brick', 'wall', 'getting', 'sleep', 'he', 'also', 'told', 'received', 'complaint', 'guest', '16th', 'floor', 'aware', 'noise', 'problem', 'why', 'place', 'u', 'floor', 'hotel', 'totally', 'booked', 'a', 'request', 'upper', 'floor', 'constitute', 'placing', 'someone', 'top', 'floor', 'using', 'request', 'justify', 'if', 'decide', 'stay', 'request', 'room', 'lower', 'floor', 'away', 'elevator', 'i', 'spoke', 'length', 'booking', 'two', 'room', 'preference', 'this', 'simply', 'poor', 'treatment', 'guest', 'believed', 'would', 'complain', '<stop>', '<start>', 'i', 'loved', 'hotel', 'the', 'room', 'chic', 'trendy', 'bed', 'comfortable', 'great', 'slipper', 'robe', 'i', 'love', 'keihl', 'bath', 'product', 'bathroom', 'we', 'went', 'birthday', 'weekend', 'card', 'plate', 'pastry', 'waiting', 'room', 'we', 'got', 'great', 'deal', 'junior', 'suite', 'travelzoo', 'tried', 'take', 'another', 'trip', 'chicago', 'deal', 'gone', 'i', 'really', 'recommend', 'hotel', 'i', 'loved', 'the', 'only', 'problem', 'david', 'burke', 'steakhouse', 'the', 'service', 'horrible', 'switched', 'order', 'table', 'next', 'compensation', 'took', '7', 'side', 'dish', 'ordered', 'nice', 'enough', 'not', 'charge', 'u', 'side', 'dish', 'did', \"n't\", 'get', 'do', \"n't\", 'go', 'go', 'lawry', 'accross', 'street', 'but', 'stay', 'james', 'btw-', '3-star', 'hotel', 'like', 'website', 'lists-', 'least', '4-star', '<stop>', '<start>', 'the', 'hard', 'rock', 'hotel', 'chicago', 'become', 'favorite', 'hotel', 'i', 'stayed', 'least', '5', 'time', 'never', 'anything', 'wonderful', 'experience', 'a', 'might', 'guessed', 'super', 'rock', 'roll', 'theme', 'music', 'paraphanelia', 'lobby', 'floor', 'elevator', 'lobby', 'the', 'room', 'large', 'photo', 'mural', 'themed', 'different', 'musician', 'you', 'request', 'floor', 'favorite', 'band', 'like', 'kiss', 'aerosmith', 'etc', 'the', 'room', 'great', 'well', 'appointed', 'super', 'comfortable', 'bed', 'luxurious', 'sheet', 'wonderous', 'pillow', 'the', 'large', 'tv', 'cool', 'sound', 'system', 'ramp', 'viewing', 'experience', 'there', 'spacious', 'desk', 'room', 'mini', 'bar', 'i', 'always', 'room', 'big', 'window', 'view', 'michigan', 'ave', 'hotel', 'sits', 'right', 'michigan', 'block', 'canal', 'the', 'bathroom', 'furnished', 'absolutely', 'great', 'fixture', 'sporting', 'great', 'design', 'shower', 'window', 'side', 'shower', 'and', 'enjoy', 'view', 'michigan', 'close', 'waterproof', 'drape', 'i', 'never', 'anything', 'great', 'experience', 'staff', 'front', 'desk', 'doorman', 'etc', 'great', 'there', 'lobby', 'bar', 'still', 'feel', 'intimate', 'nice', 'energy', 'big', 'screen', 'tv', 'catch', 'latest', 'score', 'of', 'course', 'rock', 'roll', 'music', 'pump', 'thru', 'place', 'you', 'stay', 'plain', 'hotel', 'anywhere', 'great', 'well', 'designed', 'fun', 'memorable', 'hotel', 'want', 'return', 'back', 'if', 'i', 'one', 'concern', 'would', 'rather', 'crazy', 'charge', 'parking', 'car', 'overnight', 'my', 'daily', 'car', 'parking', 'rate', 'close', '50', 'per', 'night', 'i', 'sorry', 'say', 'going', 'rate', 'loop', 'stay', 'be', 'cool', '<stop>', '<start>', 'we', 'also', 'returned', 'frigid', 'weekend', 'chicago', 'i', 'booked', 'ambassador', 'east', 'great', 'rate', '89.00', 'aaa', 'card', 'delighted', 'yes', 'room', 'show', 'sign', 'wear', 'warmth', 'charm', 'far', 'make', 'this', 'pleasant', 'change', 'sleek', 'modern', 'look', 'many', 'chain', 'boutique', 'hotel', 'we', 'became', 'member', 'i', 'preferred', 'program', 'therefore', 'eligible', 'upgrade', 'automatically', 'got', 'upon', 'check', 'we', 'lovely', 'suite', 'lot', 'sitting', 'room', 'comfortable', 'bedroom', 'bathroom', 'we', 'wonderful', 'food', 'pump', 'room', 'classic', 'our', 'oatmeal', 'tastefully', 'served', 'i', 'love', 'right', 'the', 'location', 'great', 'had', 'normal', 'winter', 'weather', 'could', 'easily', 'walked', 'michigan', 'avenue', 'shopping', 'due', 'minus', '20', 'degree', 'windchills', 'opted', 'cab', 'le', '5.00', 'fare', 'we', 'also', 'enjoyed', 'glass', 'wine', 'around', 'fireplace', 'lobby', 'found', 'welcoming', 'cozy', 'overcrowed', 'we', 'would', 'definitely', 'stay', '<stop>', '<start>', 'i', 'easily', 'say', 'one', 'worst', 'hotel', 'i', 'stayed', 'the', 'receptionist', 'front', 'snooty', 'sarcastic', 'teenager', 'post', 'teenager', 'you', 'get', 'better', 'service', 'waiter', 'waitress', 'diner', 'the', 'room', 'dirty', 'stain', 'carpet', 'showerhead', 'moldy', 'rusted', 'door', 'bathroom', 'main', 'door', 'dent', 'scratch', 'the', 'towel', 'horrible', 'aroma', 'the', 'hotel', 'narrow', 'near', 'construction', 'zone', 'i', 'would', 'plan', 'waking', '6', 'choose', 'talbott', 'the', 'next', 'day', 'i', 'asked', 'another', 'room', 'concierge', 'decent', 'guy', 'moved', 'u', 'larger', 'comfortable', 'room', 'that', 'room', 'though', 'right', 'elevator', 'torture', 'hearing', 'sound', 'elevator', 'would', 'never', 'stop', 'i', 'asked', 'late', 'checkout', '1pm', 'receptionist', 'refused', 'stating', 'would', 'charged', 'half', 'day', 'noon', 'i', 'speak', 'manager', 'gave', 'u', 'till', '1pm', 'please', 'stay', 'away', 'hotel', 'i', 'think', 'holiday', 'inn', 'would', 'better', 'andrew', 'f.', 'gulli', '<stop>', '<start>', 'my', 'wife', 'i', 'came', 'spend', 'weekend', 'downtown', 'chicago', 'shopping', 'found', 'conrad', 'hotel', 'well', 'located', 'close', 'everything', 'downtown', 'size', \"n't\", 'make', 'feel', 'lost', 'convention', 'world', 'we', 'feeling', 'staying', 'larger', 'hotel', 'past', 'our', 'suite', 'view', 'michigan', 'avenue', 'bit', 'lake', 'end', 'staff', 'professional', 'pleasant', 'friendly', 'making', 'u', 'feel', 'comfortable', 'internet', 'free', 'n', 'room', 'i', 'making', 'review', '<stop>', '<start>', 'we', 'arrived', 'thought', '350', 'night', 'would', 'decent', 'place', 'good', 'review', 'etc', 'we', 'arrived', 'find', 'website', 'picture', 'misleading', 'the', 'hotel', 'lounge', 'room', 'really', 'odd', 'dark', 'the', 'first', 'night', 'i', 'noticed', 'blanket', 'bed', 'nasty', 'looking', 'resembled', 'something', 'find', 'street', 'to', 'top', 'woke', 'night', '2', 'find', 'bed', 'bug', 'attacked', 'u', 'my', 'husband', 'i', 'bitten', 'i', 'bitten', 'way', 'husband', 'my', 'leg', 'covered', 'bite', 'i', 'ointment', 'benadryl', 'week', 'i', 'immediately', 'came', 'home', 'scalded', 'clothes', 'there', 'also', 'mold', 'around', 'tub', 'inside', 'floor', 'the', 'bathroom', 'floor', 'also', 'curling', 'tub', 'the', 'view', 'room', '607', 'roof', 'not', 'good', 'the', 'stay', 'very', 'disappointing', 'the', 'good', 'thing', 'free', 'breakfast', 'every', 'floor', '<stop>', '<start>', 'recently', 'returned', 'two-night', 'stay', 'ambassador', 'east', 'hotel', 'wa', 'looking', 'comfortable', 'hotel', 'stay', 'visiting', 'family', 'located', 'chicago', 'mission', 'accomplished', 'if', 'would', 'like', 'quiet', 'comfortable', 'hotel', 'great', 'area', 'while', 'furnishing', 'show', 'sign', 'age', 'detract', 'elegant', 'style', 'room', 'add', 'comfortable', 'home-like', 'feeling', 'found', 'staff', 'friendly', 'helpful', 'we', 'return', 'guest', 'many', 'time', 'future', 'sure', '<stop>', '<start>', 'after', 'leaving', 'important', 'document', 'room', 'i', 'called', 'asked', 'lost', 'found', 'department', 'seven', 'time', 'course', 'week', 'eventually', 'i', 'enough', 'asked', 'manager', 'put', 'hold', 'disconnected', 'finally', 'deceivingly', 'friendly', 'operator', 'promised', 'would', 'someone', 'call', 'back', 'minute', 'it', 'never', 'happened', 'so', 'avoid', 'place', 'rip', 'keep', 'later', 'they', 'trusted', 'word', '<stop>', '<start>', 'warning', 'food', 'poisoning', 'alert', 'the', 'room', 'dirty', 'shower', 'mold', 'handle', 'shower', 'daughter', 'cracked', 'head', 'open', 'the', 'elevator', 'absolutly', 'terrible', 'made', 'dizzy', 'every', 'time', 'went', 'they', 'hade', 'pasta', 'night', 'put', 'bowl', 'noodle', 'sauce', 'couple', 'roll', 'well', 'get', 'pay', 'what', 'worse', 'sauce', 'must', 'bad', 'meat', 'family', 'got', 'sick', 'the', 'one', 'thing', 'nice', 'location', 'right', 'michigan', 'avenue', 'the', 'second', 'night', 'left', 'went', 'another', 'hotel', '80', 'buck', 'cheaper', '100', 'time', 'nicer', 'do', 'not', 'stay', 'here', '<stop>', '<start>', 'i', 'took', 'wife', 'kid', 'chicago', 'one', 'last', 'fling', 'school', 'started', 'back', 'when', 'i', 'checked', 'staff', 'attentive', 'i', 'showed', 'little', 'consideration', 'friendly', 'attitude', 'small', 'tip', 'upgraded', 'huge', 'upper', 'floor', 'room', 'complete', 'bar', 'i', \"n't\", 'get', 'priceline', 'glare', 'paid', 'involved', 'the', 'air', 'water', 'show', 'underway', 'lake', 'front', 'plane', 'flying', 'room', 'the', 'kid', 'thought', 'great', 'that', 'night', 'great', 'view', 'firework', 'navy', 'pier', 'the', 'sunday', 'morning', 'breakfast', 'buffet', 'hotel', 'good', 'price']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---PREPROCESSED VALIDATION DATA---\")\n",
        "print(validation_data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VUeK8nk__ry",
        "outputId": "0000fb24-4231-47cb-e906-97d28723df69"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---PREPROCESSED VALIDATION DATA---\n",
            "['<start>', 'i', 'stayed', 'four', 'night', 'attending', 'conference', 'the', 'hotel', 'great', 'spot', 'easy', 'walk', 'michigan', 'ave', 'shopping', 'rush', 'st.', 'busy', 'street', 'the', 'room', 'i', 'spacious', 'well-appointed', 'the', 'staff', 'friendly', 'fitness', 'center', 'huge', 'well-equipped', 'clean', 'i', 'stayed', 'number', 'hotel', 'chicago', 'one', 'favorite', 'internet', \"n't\", 'free', '10', '24', 'hour', 'cheaper', 'business', 'hotel', 'worked', 'well', '<stop>', '<start>', 'love', 'location', 'proximity', 'everything', 'the', 'staff', 'friendly', 'courteous', 'they', 'nice', '2.5', 'year', 'old', 'boy', 'got', 'backpack', 'full', 'goody', 'moment', 'arrived', 'we', 'got', 'free', 'wifi', 'morning', 'drink', 'signing', 'select', 'guest', 'program', 'ca', \"n't\", 'beat', 'minor', 'issue', 'elevator', 'take', '2', 'separate', 'elevator', 'trip', 'get', 'room', 'it', 'got', 'little', 'annoying', 'going', 'often', 'otherwise', 'great', 'stay', '<stop>', '<start>', 'i', 'want', 'issue', 'travel-warning', 'folk', 'might', 'sign', 'weekend', 'deal', 'offer', 'travelzoo', 'time', 'time', 'the', 'deal', 'say', 'free', 'breakfast', 'included', 'price', 'however', \"n't\", 'tell', 'breakfast', 'consists', 'cup', 'coffee', 'bisquit', 'two', 'moreover', 'need', 'ask', 'ticket', 'lobby', 'check', 'wo', \"n't\", 'give', 'automatically', 'we', 'stayed', 'christmas', 'i', 'noticed', 'several', 'guest', 'bought', 'package', 'rather', 'unpleasant', 'experience', 'the', 'hotel', 'nice', 'though', \"n't\", 'consider', 'lousy', 'service', '<stop>', '<start>', 'just', 'got', 'back', 'three', 'night', 'knickerbocker', 'went', 'chicago', 'last', 'minute', 'christmas', 'shopping', 'hotel', 'great', 'location', 'north', 'end', 'michigan', 'ave.', 'half', 'block', 'away', 'staff', 'nice', 'professional', 'everyone', 'came', 'contact', 'helpful', 'granted', 'older', 'hotel', 'room', 'bigger', 'closet', 'we', 'booked', 'standard', 'room', 'arrived', 'upgraded', 'deluxe', 'room', 'good', 'size', 'room', 'nice', 'furnishing', 'comfortable', 'bed', 'bathroom', 'somewhat', 'small', 'housekeeping', 'great', 'job', 'cleaning', 'room', 'every', 'day', 'did', 'eat', 'restaurant', 'nix', 'saw', 'several', 'people', 'getting', 'order', 'delivered', 'bar', 'overheard', 'comment', 'saying', 'food', 'good', 'drink', 'service', 'martini', 'bar', 'slow', 'martini', 'run', '9', 'very', 'good', 'drink', \"n't\", 'shy', 'away', 'liquor', 'valet', 'parking', '35', 'day', 'in/out', 'priviliges', 'overall', 'great', 'stay', 'would', 'definately', 'stay', 'hotel', 'future', '<stop>', '<start>', 'my', 'family', 'i', 'stayed', 'hotel', 'extended', 'weekend', 'trip', 'july', '08', 'we', '3', '1', 'bedroom', 'suite', 'great', 'location', 'downtown', 'chicago', 'within', 'walking', 'distance', 'shopping', 'downtown', 'tourist', 'attraction', 'the', 'service', 'excellent', 'hotel', 'staff', 'friendly', 'the', 'breakfast', 'buffet', 'good', 'eating', 'area', 'get', 'crowded', 'i', 'recommend', 'going', 'early', 'the', 'room', 'quiet', 'clean', 'i', 'highly', 'recommend', 'hotel', 'family', 'traveler', 'staying', '2-3', 'day', 'we', 'would', 'stay', 'parking', 'expensive', '28/day', 'it', 'valet', 'the', 'cheapest', 'self', 'parking', 'i', 'could', 'find', 'area', '24/days', 'valet', 'parking', 'the', 'valet', 'bell', 'staff', 'helpful', 'friendly', 'this', '3rd', 'experience', 'homewood', 'suite', '3', 'different', 'city', 'i', 'great', 'experience', 'time', '<stop>', '<start>', 'valeted', 'lexus', 'returned', 'smashed', 'side', 'mirror', 'they', 'admitted', 'fault', 'wo', \"n't\", 'pay', '500', 'fix', 'ca', \"n't\", 'prove', 'broken', 'hotel', 'wonderful', 'room', 'new', 'nice', 'never', 'stay', 'sure', 'inspect', 'car', 'leave', '<stop>', '<start>', 'great', 'expectation', 'hotel', 'the', 'fugitive', 'wonderful', 'lobby', 'and', 'that', 'it', 'the', 'room', 'old', 'style', 'furniture', 'tv', 'century', 'minibar', 'wireless', 'internet', 'wireless', 'adapter', 'enough', 'number', 'room', 'guy', 'ignore', 'era', 'ipad', 'without', 'cable', 'connection', 'small', 'room', 'small', 'bathroom', 'rude', 'service', 'room', 'lost', 'credit', 'card', 'number', 'want', 'cash', 'on', 'delivery', 'breakfast', '20', 'dollar', 'robbery', 'business', 'center', '0,59', 'cent', 'minute', 'internet', 'connection', 'pay', 'use', 'word', 'excel', 'never', 'again', '<stop>', '<start>', 'named', 'price', 'priceline', '50.00', 'buck', 'hotel', 'room', 'great', 'clean', 'clean', 'new', 'fresh', 'crisp', 'sheet', 'comfortable', 'bed', 'flat', 'screen', 'tv', 'clean', 'carpet', 'nice', 'bath', 'etc', 'short', 'distance', 'food', 'sightseeing', 'i', 'highly', 'recommend', 'property', 'be', 'prepared', 'pay', '40.00', 'per', 'night', 'park', 'hey', 'it', 'hyatt', 'they', 'also', 'charge', '5.00', 'bottle', 'water', 'normally', 'buck', 'for', 'total', '100.00', 'nightly', 'stay', 'chicago', 'place', \"n't\", 'drink', 'bottled', 'water', 'happy', 'travel', '<stop>', '<start>', 'i', 'never', 'write', 'review', 'felt', 'important', 'state', 'hotel', '1', '2008', 'traveler', 'choice', 'terrible', 'service', 'lazy', 'doormen/bellmen/concierge/valet', 'lack', 'respect', 'allow', 'dog', 'bark', 'morning', 'multiple', 'complaint', 'phantom', 'charge', 'room', 'front', 'dest', 'checkout', 'checkin', 'pompus', 'the', 'icing', 'cake', 'watching', 'doorman', 'stand', 'watch', 'father', 'tried', 'open', 'door', 'push', 'stroller', 'infant', 'son', 'doorman', 'watched', 'nothing', 'me', 'girlfriend', 'young', 'look', 'young', 'dress', 'honesty', 'make', 'money', 'treated', 'like', 'cheap', 'poor', 'kid', 'i', 'dinner', 'reservation', 'spaiggia', 'wanted', 'order', 'car', 'pick', 'u', 'concierge', 'exclaimed', 'you', 'know', 'jean', 'running', 'shoe', 'right', 'mr', 'blank', 'assuming', 'i', 'dumb', 'realize', '5', 'star', '200/person', 'restraunt', 'wont', 'allow', 'tennis', 'shoe', 'i', 'young', 'very', 'dissapointed', 'very', 'nice', 'looking', 'comfortable', 'bed', 'great', 'room', 'service', 'contemporary', 'younger', 'crowd', 'service', 'terrible', '<stop>', '<start>', 'my', 'husband', 'i', 'stayed', 'three', 'night', 'visiting', 'chicago', 'tourist', 'what', 'beautiful', 'hotel', 'our', 'room', 'lovely', 'staff', 'super', 'helpful', 'location', 'convenient', 'they', \"n't\", 'house', 'gym', 'day', 'get', 'day', 'pas', 'cushy', 'gym', 'street', 'the', 'restaurant', 'hotel', 'yummy', 'breakfast', 'the', 'concierge', 'helped', 'u', 'make', 'dinner', 'reservation', 'last', 'minute', 'first', 'night', 'we', 'able', 'walk', 'magnificent', 'mile', 'hotel', 'well', 'gold', 'coast', 'old', 'towne', 'neighborhood', 'there', 'ton', 'great', 'restaurant', 'area', 'including', 'famous', 'gibson', 'signature', 'room', 'hancock', 'tower', 'well', 'lot', 'shopping', 'there', 'currently', 'lot', 'construction', 'going', 'near', 'talbott', 'building', 'condo', 'right', 'behind', 'regardless', 'wonderful', 'stay', 'would', 'definitely', 'stay', 'visiting', 'chicago', '<stop>', '<start>', 'this', 'second', 'time', 'two', 'month', 'stay', 'great', 'the', 'room', 'nice', 'clean', 'the', 'staff', 'terrific', 'always', 'attentive', 'my', 'concern', 'would', 'never', 'seems', 'anyone', 'concierge', 'desk', '<stop>', '<start>', 'if', 'i', \"n't\", 'stay', 'i', 'would', \"n't\", 'without', 'exception', 'i', 'swear', 'i', 'lying', 'every', 'day', 'stay', 'brings', 'new', 'problem', 'over', 'past', '3', 'month', 'i', 'stayed', '6', 'time', '3', 'night', 'time', 'problem', 'every', 'time', 'from', 'wrong', 'food', 'room', 'service', 'room', 'key', 'dont', 'work', 'unrefreshed', 'bath', 'towel', 'forgotten', 'dirty', 'in-room', 'coffee', 'service', 'unclean', 'ice', 'bucket', 'overnight', 'forgotten', 'room', 'cleaning', 'inability', 'connect', 'internet', 'tired', 'keep', 'writing', 'problem', 'for', 'price', 'chi-chi', 'downtown', 'hotel', 'i', 'expect', 'finally', 'team', 'certainly', 'trained', 'customer', 'service', 'the', 'front', 'desk', 'team', 'attitude', 'aria', 'hostess', 'think', 'something', 'else', 'i', 'stuck', 'using', 'hotel', 'since', 'adjoined', 'aon', 'i', 'would', 'never', 'go', 'back', 'i', 'choice', 'the', 'good', 'thing', 'food', 'aria', 'outstanding', '<stop>', '<start>', 'just', 'back', 'spending', 'memorial', 'day', 'weekend', 'chicago', 'we', 'decided', 'stay', 'talbott', 'reserved', 'room', 'month', 'ago', 'i', 'must', 'say', 'found', 'staff', 'wonderful', 'accommodation', 'quite', 'disappointing', 'we', 'given', 'room', '1605', 'avoid', 'room', 'cost', 'the', 'adjacent', 'elevator', 'noisy', 'keep', 'awake', 'night', 'awaken', '6', 'am', 'in', 'addition', 'requested', 'king', 'bed', '1605', 'two', 'double', 'we', 'also', 'surprised', 'coffee', 'maker', 'room', 'despite', 'friendly', 'staff', 'super', 'location', 'talbott', 'le', 'expected', 'would', 'probably', 'stay', '<stop>', '<start>', 'i', 'believe', 'i', 'describe', 'amazing', 'stay', 'the', 'james', 'service', 'impecable', 'door', 'man', 'front', 'desk', 'location', 'could', 'better', 'yes', 'hear', 'ambulance', 'street', '14th', 'floor', 'come', 'downtown', 'expect', 'noise', 'level', 'bother', 'u', 'the', 'minor', 'incident', 'cleaner', 'tried', 'open', 'room', 'clean', '9', 'pm', 'pm', 'am', 'go', 'figure', '<stop>', '<start>', 'my', 'husband', 'i', 'decided', 'take', 'trip', 'chicago', 'last', 'minute']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unigram Model Implementation"
      ],
      "metadata": {
        "id": "YRftCgk-85Cr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zDut8ufBRn5L"
      },
      "outputs": [],
      "source": [
        "class Unigram:\n",
        "    def __init__(self, data, smoothing_technique = \"k-smooth\", k = 0):\n",
        "        self.data = data\n",
        "        self.word_count  = defaultdict(int)\n",
        "        self.probabilities = defaultdict(float)\n",
        "        self.total_word_count = 0\n",
        "        self.smoothing_technique = smoothing_technique\n",
        "        self.unique_words = set()\n",
        "        self.k = k\n",
        "\n",
        "    def count_words_from_corpus(self):\n",
        "        self.word_count = defaultdict(int)\n",
        "        self.total_word_count = 0\n",
        "        for e in self.data:\n",
        "            self.word_count[e] += 1\n",
        "            self.total_word_count += 1\n",
        "\n",
        "    def calculate_probabilties(self):\n",
        "        self.probabilities = defaultdict(float, {key: value / self.total_word_count for key, value in self.word_count.items()})\n",
        "\n",
        "    def train(self):\n",
        "        self.count_words_from_corpus()\n",
        "        self.calculate_probabilties()\n",
        "        self.unique_words = set(self.word_count.keys())\n",
        "\n",
        "    def smooth(self):\n",
        "        # Updating probabilities of the word already tracked by the model\n",
        "        if self.smoothing_technique == \"k-smooth\":\n",
        "            for e in self.word_count:\n",
        "                self.probabilities[e] = (self.word_count[e] + self.k) / (self.total_word_count + (self.k * len(self.unique_words)))\n",
        "\n",
        "        elif self.smoothing_technique == \"laplace\":\n",
        "            for e in self.word_count:\n",
        "                self.probabilities[e] = (self.word_count[e] + 1) / (self.total_word_count + len(self.unique_words))\n",
        "\n",
        "    def handle_zero_probability(self, token):\n",
        "        if self.smoothing_technique == \"k-smooth\":\n",
        "            return (self.word_count[token] + self.k) / (self.total_word_count + (self.k * len(self.unique_words)))\n",
        "\n",
        "        elif self.smoothing_technique == \"laplace\":\n",
        "            return (self.word_count[token] + 1) / (self.total_word_count + len(self.unique_words))\n",
        "\n",
        "        elif self.smoothing_technique == \"unk\":\n",
        "            return self.word_count[\"<unk>\"] / self.total_word_count\n",
        "\n",
        "    def handle_unknown(self, n = 1):\n",
        "        unk_words = [key for key, val in self.word_count.items() if val <= n]\n",
        "        self.data = [\"<unk>\" if e in unk_words else e for e in self.data]\n",
        "        self.train()\n",
        "\n",
        "    def perplexity(self, data):\n",
        "        total_log_val = 0\n",
        "        for word in data:\n",
        "            if word in self.unique_words:\n",
        "                total_log_val += math.log2(self.probabilities[word])\n",
        "            else:\n",
        "                total_log_val += self.handle_zero_probability(word)\n",
        "\n",
        "        total_log_val /= len(data)\n",
        "        return 2 ** -total_log_val"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bigram Model Implementation"
      ],
      "metadata": {
        "id": "TafP-rcqGho_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rGZ-dkiiZyoy"
      },
      "outputs": [],
      "source": [
        "class Bigram:\n",
        "    def __init__(self, data, smoothing_technique = \"k-smooth\", k = 0):\n",
        "        self.data = data\n",
        "        self.word_count = defaultdict(int)\n",
        "        self.bigram_word_count = defaultdict(int)\n",
        "        self.probabilties = defaultdict(float)\n",
        "        self.total_word_count = 0\n",
        "        self.smoothing_technique = smoothing_technique\n",
        "        self.k = k\n",
        "        self.unique_words = set()\n",
        "        self.unique_bigram_words = set()\n",
        "\n",
        "    def count_words_from_corpus(self):\n",
        "        self.word_count.clear()\n",
        "        self.total_word_count = 0\n",
        "        self.bigram_word_count.clear()\n",
        "\n",
        "        for sentence in self.data:\n",
        "            for word in sentence:\n",
        "                self.word_count[word] += 1\n",
        "                self.total_word_count += 1\n",
        "\n",
        "        for sentence in self.data:\n",
        "            for bigram in zip(sentence, sentence[1:]):\n",
        "                if bigram == (\"they\", \"wo\"):\n",
        "                    print(True)\n",
        "                self.bigram_word_count[bigram] += 1\n",
        "\n",
        "    def calculate_probabilties(self):\n",
        "        self.probabilties = defaultdict(float, {key: self.bigram_word_count[(key[0], key[1])] / self.word_count[key[0]] for key, value in self.bigram_word_count.items()})\n",
        "\n",
        "    def train(self):\n",
        "        self.count_words_from_corpus()\n",
        "        self.calculate_probabilties()\n",
        "        self.unique_words = set(self.word_count.keys())\n",
        "        self.unique_bigram_words = set(self.bigram_word_count.keys())\n",
        "\n",
        "    def handle_zero_probability(self, token):\n",
        "        if self.smoothing_technique == \"k-smooth\":\n",
        "            return (self.bigram_word_count[token] + self.k) / (self.word_count[token[0]] + (self.k * len(self.unique_words)))\n",
        "\n",
        "        elif self.smoothing_technique == \"laplace\":\n",
        "            return (self.bigram_word_count[token] + 1) / (self.word_count[token[0]] + len(self.unique_words))\n",
        "\n",
        "        elif self.smoothing_technique == \"unk\":\n",
        "            return self.bigram_word_count[(\"<unk>\", \"<unk>\")] / self.word_count[\"<unk>\"]\n",
        "\n",
        "    def handle_unknown(self, n = 1):\n",
        "        unk_words = [key for key, val in self.word_count.items() if val <= n]\n",
        "        self.data = [[\"<unk>\" if word in unk_words else word for word in sentence] for sentence in self.data]\n",
        "        self.train()\n",
        "\n",
        "    def smooth(self):\n",
        "        if self.smoothing_technique == \"k-smooth\":\n",
        "            for e in self.bigram_word_count:\n",
        "                self.probabilties[e] = (self.bigram_word_count[e] + self.k) / (self.word_count[e[0]] + (self.k * len(self.unique_words)))\n",
        "        elif self.smoothing_technique == \"laplace\":\n",
        "            for e in self.bigram_word_count:\n",
        "                self.probabilties[e] = (self.bigram_word_count[e] + 1) / (self.word_count[e[0]] + len(self.unique_words))\n",
        "\n",
        "    def perplexity(self, data):\n",
        "        total_log_prob = 0.0\n",
        "        for sentence in data:\n",
        "            for e in zip(sentence, sentence[1:]):\n",
        "                if e in self.unique_bigram_words:\n",
        "                    total_log_prob += math.log2(self.probabilties[e])\n",
        "                else:\n",
        "                    total_log_prob += self.handle_zero_probability(e)\n",
        "        total_log_prob = total_log_prob / sum([len(sentence) for sentence in data])\n",
        "        return 2 ** -total_log_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perplexity of Unigrams on training set\n",
        "- Unsmoothed Unigram\n",
        "- \\<unk> word handled Unigram\n",
        "- Laplace Smoothed Unigram\n",
        "- Add_k_smoothed Unigram\n",
        "    - k = 0.5\n",
        "    - k = 0.1\n",
        "    - k = 0.05\n",
        "    - k = 0.01\n"
      ],
      "metadata": {
        "id": "qn7hJxAXH5g-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unsmoothed Unigram on training set\n",
        "unigram_train = Unigram(train_data)\n",
        "unigram_train.train()\n",
        "print(\"Perplexity of Unsmoothed Unigram model on training set: \",unigram_train.perplexity(train_data))"
      ],
      "metadata": {
        "id": "vUdSEhBBn8Ds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97e81365-5909-40e7-ff81-65fb5a1139dc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Unsmoothed Unigram model on training set:  1069.5292498671135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unknown words handled Unigram on training set\n",
        "unigram_unk_train = Unigram(train_data, \"unk\")\n",
        "unigram_unk_train.train()\n",
        "unigram_unk_train.handle_unknown(n = 1)\n",
        "print(\"Perplexity of Unknown words handled Unigram model on training set: \",unigram_unk_train.perplexity(train_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhKBi7zCHwM6",
        "outputId": "fb23d333-2468-4947-ee9e-cfa4a79f9a54"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Unknown words handled Unigram model on training set:  542.7438202518388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Laplace smoothed Unigram on training set\n",
        "unigram_laplace_train = Unigram(train_data, \"laplace\")\n",
        "unigram_laplace_train.train()\n",
        "unigram_laplace_train.smooth()\n",
        "print(\"Perplexity of Laplace smoothed Unigram model on training set: \",unigram_laplace_train.perplexity(train_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8Y06XaeJCCs",
        "outputId": "9f009ec1-a109-40e6-f990-a7f083a023ca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Laplace smoothed Unigram model on training set:  1089.4927440212678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add-k-smoothed Unigram on training set with k = 0.5\n",
        "unigram_k_smooth_train = Unigram(train_data, \"k-smooth\", 0.5)\n",
        "unigram_k_smooth_train.train()\n",
        "unigram_k_smooth_train.smooth()\n",
        "print(\"Perplexity of Add-k-smoothed Unigram model on training set with k = 0.5: \",unigram_k_smooth_train.perplexity(train_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddLruW7JJZWl",
        "outputId": "0d564818-a877-4a89-c6e5-651631cad5dc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Add-k-smoothed Unigram model on training set with k = 0.5:  1075.7920268617872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add-k-smoothed Unigram on training set with k = 0.1\n",
        "unigram_k_smooth_train_2 = Unigram(train_data, \"k-smooth\", 0.1)\n",
        "unigram_k_smooth_train_2.train()\n",
        "unigram_k_smooth_train_2.smooth()\n",
        "print(\"Perplexity of Add-k-smoothed Unigram model on training set with k = 0.1: \",unigram_k_smooth_train_2.perplexity(train_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXNR9vZ2KV5e",
        "outputId": "f5c358ab-e55f-4566-c2ac-d9f29b5ea4c8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Add-k-smoothed Unigram model on training set with k = 0.1:  1069.8447365637267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add-k-smoothed Unigram on training set with k = 0.05\n",
        "unigram_k_smooth_train_3 = Unigram(train_data, \"k-smooth\", 0.05)\n",
        "unigram_k_smooth_train_3.train()\n",
        "unigram_k_smooth_train_3.smooth()\n",
        "print(\"Perplexity of Add-k-smoothed Unigram model on training set with k = 0.05: \",unigram_k_smooth_train_3.perplexity(train_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3peyblM8KxKV",
        "outputId": "bd49d0ba-a8ae-49a8-8fc3-ec4f512f1daa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Add-k-smoothed Unigram model on training set with k = 0.05:  1069.6107982808858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add-k-smoothed Unigram on training set with k = 0.01\n",
        "unigram_k_smooth_train_4 = Unigram(train_data, \"k-smooth\", 0.01)\n",
        "unigram_k_smooth_train_4.train()\n",
        "unigram_k_smooth_train_4.smooth()\n",
        "print(\"Perplexity of Add-k-smoothed Unigram model on training set with k = 0.01: \",unigram_k_smooth_train_4.perplexity(train_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obC1HRYjLA-t",
        "outputId": "6b40c398-0b71-421c-df02-6baaae56302b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Add-k-smoothed Unigram model on training set with k = 0.01:  1069.5326031837844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perplexity of Bigrams on training set\n",
        "- Unsmoothed Bigram\n",
        "- \\<unk> word handled Bigram\n",
        "- Laplace Smoothed Bigram\n",
        "- Add_k_smoothed Bigram\n",
        "    - k = 0.5\n",
        "    - k = 0.1\n",
        "    - k = 0.05\n",
        "    - k = 0.01\n"
      ],
      "metadata": {
        "id": "bpMJuxEFLKqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unsmoothed Bigram on training set\n",
        "bigram_train = Bigram(train_data_sentences)\n",
        "bigram_train.train()\n",
        "print(\"Perplexity of Unsmoothed Bigram model on training set: \",bigram_train.perplexity(train_data_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj1L6sW5LFFw",
        "outputId": "4f373fdd-b077-40f1-b083-b84e599852b4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Unsmoothed Bigram model on training set:  20.699652185270793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unknown words handled Bigram on training set\n",
        "bigram_unk_train = Bigram(train_data_sentences, \"unk\")\n",
        "bigram_unk_train.train()\n",
        "bigram_unk_train.handle_unknown(n = 1)\n",
        "print(\"Perplexity of Unknown words handled Bigram model on training set: \", bigram_unk_train.perplexity(train_data_sentences))"
      ],
      "metadata": {
        "id": "1D6d4S0LLUWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0876fc19-0c6f-4d40-c46f-7a44e6d292c1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Unknown words handled Bigram model on training set:  16.78079536741834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Laplace smoothed Bigram on training set\n",
        "bigram_laplace_train = Bigram(train_data_sentences, \"laplace\")\n",
        "bigram_laplace_train.train()\n",
        "bigram_laplace_train.smooth()\n",
        "print(\"Perplexity of Laplace smoothed Bigram model on training set: \", bigram_laplace_train.perplexity(train_data_sentences))"
      ],
      "metadata": {
        "id": "rhFxktcQLfkp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4bdfb68-f72e-480e-8e8b-973b0d7fcb98"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Laplace smoothed Bigram model on training set:  1667.735448912925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add-k-smoothed Bigram on training set with k = 0.5\n",
        "bigram_add_k_train = Bigram(train_data_sentences, \"k-smooth\", 0.5)\n",
        "bigram_add_k_train.train()\n",
        "bigram_add_k_train.smooth()\n",
        "print(\"Perplexity of Add-k-smoothed Bigram model on training set with k = 0.5: \", bigram_add_k_train.perplexity(train_data_sentences))"
      ],
      "metadata": {
        "id": "mYB9abc3RKC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b46771-a54b-45c1-891a-421127e0c51a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Add-k-smoothed Bigram model on training set with k = 0.5:  1074.6482403803805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add-k-smoothed Bigram on training set with k = 0.1\n",
        "bigram_add_k_train_2 = Bigram(train_data_sentences, \"k-smooth\", 0.1)\n",
        "bigram_add_k_train_2.train()\n",
        "bigram_add_k_train_2.smooth()\n",
        "print(\"Perplexity of Add-k-smoothed Bigram model on training set with k = 0.1: \", bigram_add_k_train_2.perplexity(train_data_sentences))"
      ],
      "metadata": {
        "id": "DuE7GDevRl4S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7872bc89-6b7c-40bc-8f68-7f8ca7439026"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Add-k-smoothed Bigram model on training set with k = 0.1:  324.37033322826727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add-k-smoothed Bigram on training set with k = 0.05\n",
        "bigram_add_k_train_3 = Bigram(train_data_sentences, \"k-smooth\", 0.05)\n",
        "bigram_add_k_train_3.train()\n",
        "bigram_add_k_train_3.smooth()\n",
        "print(\"Perplexity of Add-k-smoothed Bigram model on training set with k = 0.05: \", bigram_add_k_train_3.perplexity(train_data_sentences))"
      ],
      "metadata": {
        "id": "beHLh7gfR8ep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fe92cf5-4739-49c4-e1ee-6905eb22f47c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Add-k-smoothed Bigram model on training set with k = 0.05:  193.7987065845242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add-k-smoothed Bigram on training set with k = 0.01\n",
        "bigram_add_k_train_4 = Bigram(train_data_sentences, \"k-smooth\", 0.01)\n",
        "bigram_add_k_train_4.train()\n",
        "bigram_add_k_train_4.smooth()\n",
        "print(\"Perplexity of Add-k-smoothed Bigram model on training set with k = 0.01:\", bigram_add_k_train_4.perplexity(train_data_sentences))"
      ],
      "metadata": {
        "id": "WXko6VIdSJNv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22b33e71-7569-46aa-ae05-73221ec21ecf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Add-k-smoothed Bigram model on training set with k = 0.01: 69.65575369691636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perplexity of Unigrams on validation set\n",
        "- \\<unk> word handled Unigram\n",
        "- Laplace Smoothed Unigram\n",
        "- Add_k_smoothed Unigram\n",
        "    - k = 0.5\n",
        "    - k = 0.1\n",
        "    - k = 0.05\n",
        "    - k = 0.01\n"
      ],
      "metadata": {
        "id": "xQn4W7zjSUKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unknown words handled Unigram on training set\n",
        "unigram_unk_val = Unigram(train_data, \"unk\")\n",
        "unigram_unk_val.train()\n",
        "unigram_unk_val.handle_unknown(n = 1)\n",
        "print(\"Perplexity of Unknown words handled Unigram model on validation set: \",unigram_unk_val.perplexity(validation_data))"
      ],
      "metadata": {
        "id": "uxEvydhpTPzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aabe9bc-9eb4-4249-d55a-3644b695e540"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Unknown words handled Unigram model on validation set:  396.5196287422726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Laplace smoothed Unigram on training set\n",
        "unigram_laplace_val = Unigram(train_data, \"laplace\")\n",
        "unigram_laplace_val.train()\n",
        "unigram_laplace_val.smooth()\n",
        "print(\"Perplexity of Laplace smoothed Unigram model on validation set: \",unigram_laplace_val.perplexity(validation_data))"
      ],
      "metadata": {
        "id": "8xmlFb7UTPzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f8afddd-03be-483a-ed23-3d7fad71ba7f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Laplace smoothed Unigram model on validation set:  601.440045631427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add-k-smoothed Unigram on training set with k = 0.5\n",
        "unigram_k_smooth_val = Unigram(train_data, \"k-smooth\", 0.5)\n",
        "unigram_k_smooth_val.train()\n",
        "unigram_k_smooth_val.smooth()\n",
        "print(\"Perplexity of Add-k-smoothed Unigram model on validation set with k = 0.5: \",unigram_k_smooth_val.perplexity(validation_data))"
      ],
      "metadata": {
        "id": "vxNXdJVKTPzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a32918e4-67c3-4118-e61b-507720e3ecbd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Add-k-smoothed Unigram model on validation set with k = 0.5:  588.4087698679492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add-k-smoothed Unigram on training set with k = 0.1\n",
        "unigram_k_smooth_val_2 = Unigram(train_data, \"k-smooth\", 0.1)\n",
        "unigram_k_smooth_val_2.train()\n",
        "unigram_k_smooth_val_2.smooth()\n",
        "print(\"Perplexity of Add-k-smoothed Unigram model on validation set with k = 0.1: \",unigram_k_smooth_val_2.perplexity(validation_data))"
      ],
      "metadata": {
        "id": "5dbyzJzdTPzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dcae737-6f73-4ce4-e35e-bbf56136165b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Add-k-smoothed Unigram model on validation set with k = 0.1:  579.3317189118854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add-k-smoothed Unigram on training set with k = 0.05\n",
        "unigram_k_smooth_val_3 = Unigram(train_data, \"k-smooth\", 0.05)\n",
        "unigram_k_smooth_val_3.train()\n",
        "unigram_k_smooth_val_3.smooth()\n",
        "print(\"Perplexity of Add-k-smoothed Unigram model on validation set with k = 0.05: \",unigram_k_smooth_val_3.perplexity(validation_data))"
      ],
      "metadata": {
        "id": "yGd85fJtTPzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9549eaf8-1e60-4987-e568-225df06305d9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Add-k-smoothed Unigram model on validation set with k = 0.05:  578.3382924081207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add-k-smoothed Unigram on training set with k = 0.01\n",
        "unigram_k_smooth_val_4 = Unigram(train_data, \"k-smooth\", 0.01)\n",
        "unigram_k_smooth_val_4.train()\n",
        "unigram_k_smooth_val_4.smooth()\n",
        "print(\"Perplexity of Add-k-smoothed Unigram model on validation set with k = 0.01: \",unigram_k_smooth_val_4.perplexity(validation_data))"
      ],
      "metadata": {
        "id": "Vr5SDkUaTPzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50215df6-038d-4248-c07b-440d49b735e7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Add-k-smoothed Unigram model on validation set with k = 0.01:  577.5740554105223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perplexity of Bigrams on validation set\n",
        "- \\<unk> word handled Bigram\n",
        "- Laplace Smoothed Bigram\n",
        "- Add_k_smoothed Bigram\n",
        "    - k = 0.5\n",
        "    - k = 0.1\n",
        "    - k = 0.05\n",
        "    - k = 0.01\n"
      ],
      "metadata": {
        "id": "NC3ZBG-oTvEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unknown words handled Bigram on validation set\n",
        "bigram_unk_val = Bigram(train_data_sentences, \"unk\")\n",
        "bigram_unk_val.train()\n",
        "bigram_unk_val.handle_unknown(n = 1)\n",
        "print(\"Perplexity of Unknown words handled Bigram model on validation set: \", bigram_unk_val.perplexity(validation_data_sentences))"
      ],
      "metadata": {
        "id": "0q97qI8ATvEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "908784bc-c9c3-4227-e71d-9531754b8bdd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Unknown words handled Bigram model on validation set:  3.51489086085866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Laplace smoothed Bigram on validation set\n",
        "bigram_laplace_val = Bigram(train_data_sentences, \"laplace\")\n",
        "bigram_laplace_val.train()\n",
        "bigram_laplace_val.smooth()\n",
        "print(\"Perplexity of Unknown words handled Bigram model on validation set: \", bigram_laplace_val.perplexity(validation_data_sentences))"
      ],
      "metadata": {
        "id": "_3dCqO-lTvEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56df17fe-7067-4f3a-f2c3-6741e6811333"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Unknown words handled Bigram model on validation set:  14.613967735058832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add-k-smoothed Bigram on validation set with k = 0.5\n",
        "bigram_add_k_val = Bigram(train_data_sentences, \"k-smooth\", 0.5)\n",
        "bigram_add_k_val.train()\n",
        "bigram_add_k_val.smooth()\n",
        "print(\"Perplexity of Add-k-smoothed Bigram model on validation set with k = 0.5: \", bigram_add_k_val.perplexity(validation_data_sentences))"
      ],
      "metadata": {
        "id": "49ib1BY9f9bS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f88b7bc-ca74-4e62-b9aa-0594a907ffad"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Add-k-smoothed Bigram model on validation set with k = 0.5:  12.032963366017078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add-k-smoothed Bigram on validation set with k = 0.1\n",
        "bigram_add_k_val_2 = Bigram(train_data_sentences, \"k-smooth\", 0.1)\n",
        "bigram_add_k_val_2.train()\n",
        "bigram_add_k_val_2.smooth()\n",
        "print(\"Perplexity of Add-k-smoothed Bigram model on validation set with k = 0.1: \", bigram_add_k_val_2.perplexity(validation_data_sentences))"
      ],
      "metadata": {
        "id": "V4_zC3Gqf9bS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a3d9b03-56e1-48f6-e71e-150f8e1e3bc9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Add-k-smoothed Bigram model on validation set with k = 0.1:  7.605518902893464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add-k-smoothed Bigram on validation set with k = 0.05\n",
        "bigram_add_k_train_3 = Bigram(train_data_sentences, \"k-smooth\", 0.05)\n",
        "bigram_add_k_train_3.train()\n",
        "bigram_add_k_train_3.smooth()\n",
        "print(\"Perplexity of Add-k-smoothed Bigram model on validation set with k = 0.05: \", bigram_add_k_train_3.perplexity(validation_data_sentences))"
      ],
      "metadata": {
        "id": "2O-mYPW0f9bS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64cfda96-6c1a-4ce4-d14a-3e0cece51e37"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Add-k-smoothed Bigram model on validation set with k = 0.05:  6.39367695892385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add-k-smoothed Bigram on validation set with k = 0.01\n",
        "bigram_add_k_val_4 = Bigram(train_data_sentences, \"k-smooth\", 0.01)\n",
        "bigram_add_k_val_4.train()\n",
        "bigram_add_k_val_4.smooth()\n",
        "print(\"Perplexity of Add-k-smoothed Bigram model on validation set with k = 0.01: \", bigram_add_k_val_4.perplexity(validation_data_sentences))"
      ],
      "metadata": {
        "id": "XC919YCMf9bT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6d3ca9d-e3a0-4baa-ce61-29850d245b81"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Add-k-smoothed Bigram model on validation set with k = 0.01:  4.731323620901083\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}